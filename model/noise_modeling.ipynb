{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import geopy.distance\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point\n",
    "import shapely\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn import metrics\n",
    "import folium\n",
    "\n",
    "import math\n",
    "\n",
    "from descartes import PolygonPatch\n",
    "import altair as alt\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# for the notebook only (not for JupyterLab) run this command once per session\n",
    "alt.renderers.enable('notebook')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Sensors datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Consts\n",
    "datapath = '../rawdata/sensors/'\n",
    "filename = datapath + 'nodes.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading external datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading 311\n",
    "noiseComplaints = pd.read_pickle('../data/311/311.pkl')\n",
    "\n",
    "## loading taxi\n",
    "taxi = pd.read_pickle('../data/taxi/taxi.pkl')\n",
    "\n",
    "## loading wind speed\n",
    "windSpeed = pd.read_pickle('../data/weather/wind.pkl')\n",
    "windSpeed = windSpeed.resample('H').agg({'Spd[Wind]': 'mean'})\n",
    "\n",
    "## loading precipitation\n",
    "precipitation = pd.read_pickle('../data/weather/precipitation.pkl')\n",
    "precipitation = precipitation.resample('H').agg({'Amt[PrecipHourly1]': 'mean'})\n",
    "\n",
    "## loading bus\n",
    "# bus = pd.read_pickle('../data/bus/bus.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the largest intersection across datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noiseComplaints_start, noiseComplaints_end = noiseComplaints.index[0], noiseComplaints.index[-1]\n",
    "taxi_start, taxi_end = taxi.index[0], taxi.index[-1]\n",
    "windSpeed_start, windSpeed_end = windSpeed.index[0], windSpeed.index[-1]\n",
    "precipitation_start, precipitation_end = precipitation.index[0], precipitation.index[-1]\n",
    "# bus_start, bus_end = bus.index[0], bus.index[-1]\n",
    "\n",
    "## Calculating the largest intersection\n",
    "intersection_start = max(noiseComplaints_start, taxi_start, windSpeed_start, precipitation_start)\n",
    "intersection_end = min(noiseComplaints_end, taxi_end, windSpeed_end, precipitation_end)\n",
    "\n",
    "print('311 Range: ', noiseComplaints_start, '----', noiseComplaints_end)\n",
    "print('Taxi Range: ', taxi_start, '----', taxi_end)\n",
    "print('Wind Speed Range: ', windSpeed_start,'----', windSpeed_end)\n",
    "print('Rain Precipitation Range: ', precipitation_start,'----', precipitation_end)\n",
    "# print('Bus Range: ', bus_start,'----', bus_end)\n",
    "print('Largest Intersection: ', intersection_start,'----', intersection_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load sensors\n",
    "selectedsensors = [ \"sonycnode-b827eb0fedda.sonyc\",\n",
    "                    \"sonycnode-b827eb42bd4a.sonyc\",\n",
    "                    \"sonycnode-b827eb44506f.sonyc\",\n",
    "                    \"sonycnode-b827eb73e772.sonyc\",\n",
    "#                     \"sonycnode-b827eb74a519.sonyc\",\n",
    "                    \"sonycnode-b827eb84deb5.sonyc\",\n",
    "                    \"sonycnode-b827ebb40450.sonyc\"]\n",
    "\n",
    "# selectedsensors = [ \n",
    "# #                     \"sonycnode-b827eb18a94a.sonyc\",\n",
    "#                     \"sonycnode-b827eb429cd4.sonyc\",\n",
    "#                     \"sonycnode-b827eb977bfb.sonyc\"]\n",
    "\n",
    "f = open(filename)\n",
    "sensors = {}\n",
    "\n",
    "for line in f:\n",
    "    s, lat, lon = line.split(' ')\n",
    "    if s in selectedsensors:\n",
    "        \n",
    "        print('Collecting for sensor ', s)\n",
    "        \n",
    "        # collection sensor metadata\n",
    "        sensors[s] = {}\n",
    "        sensors[s]['lat'] = float(lat)\n",
    "        sensors[s]['lon'] = float(lon)\n",
    "        \n",
    "        # calculating the taxi region to which the sensor belongs\n",
    "        sensors[s]['taxi_region'] = getSensorLocation(sensors[s]['lat'], sensors[s]['lon'])\n",
    "        \n",
    "        # loading sensor data\n",
    "        sensorData = pd.read_pickle(datapath +s+ '.pkl')\n",
    "        \n",
    "        # calculating the intersection with the external datasets\n",
    "        sensorData_start, sensorData_end = sensorData.index[0], sensorData.index[-1]\n",
    "        dataframe_start, dataframe_end = max(sensorData_start, intersection_start), min(sensorData_end, intersection_end)\n",
    "        \n",
    "        # creating empty timeseries\n",
    "        df_timeseries = pd.DataFrame()\n",
    "        df_timeseries['datetime'] = pd.date_range(dataframe_start, dataframe_end, freq=\"1h\")\n",
    "        df_timeseries.set_index(['datetime'], inplace = True)\n",
    "        \n",
    "        # calculating the average over one hour of SPL\n",
    "        sensorData['dbas'] = sensorData['sum'] / sensorData['count']\n",
    "        \n",
    "        # adding sensor data to the empty dataframe\n",
    "        df_timeseries['dbas'] = sensorData['dbas'][dataframe_start:dataframe_end]\n",
    "        \n",
    "        # adding wind speed to the dataframe\n",
    "        df_timeseries['wind'] = windSpeed[dataframe_start:dataframe_end]\n",
    "        \n",
    "        # adding rain precipitation to the dataframe\n",
    "        df_timeseries['precipitation'] = precipitation[dataframe_start:dataframe_end]\n",
    "        \n",
    "        print('\\t -Adding 311 data...')\n",
    "        # adding 311 data to the empty dataframe\n",
    "        ############################\n",
    "        #SPEEDING UP FOR THIS CASE#\n",
    "        ###########################\n",
    "        #################################################################\n",
    "        noiseComplaints_temp = noiseComplaints['2017-12-31':'2018-12-31']\n",
    "        #################################################################\n",
    "        noiseComplaints_temp = noiseComplaints_temp[noiseComplaints_temp.apply(lambda row: pointWithinCircle([row['Latitude'], row['Longitude']], [lat, lon, 200]), axis=1)]\n",
    "        noiseComplaints_temp = noiseComplaints_temp.resample('H').agg({'Descriptor': 'count'})\n",
    "        noiseComplaints_temp.rename({'Descriptor':'noise'}, inplace=True)\n",
    "        df_timeseries['noise'] = noiseComplaints_temp[dataframe_start:dataframe_end]\n",
    "        \n",
    "        print('\\t -Adding taxi data...')\n",
    "        # adding taxi data to the empty dataframe\n",
    "        taxi_temp = taxi[taxi['location'] == sensors[s]['taxi_region']]\n",
    "        taxi_temp = taxi_temp.resample('H').agg({'location': 'count'})\n",
    "        taxi_temp.rename({'location':'trips'}, inplace=True)\n",
    "        df_timeseries['taxi'] = taxi_temp[dataframe_start:dataframe_end]\n",
    "                \n",
    "        # filling the missing entries with 0\n",
    "        df_timeseries.fillna(0, inplace=True)\n",
    "        df_timeseries['dbas'] = df_timeseries['dbas'].astype(int)\n",
    "        \n",
    "        # adding cos and sin to the dataframe\n",
    "        df_timeseries['hour'] = df_timeseries.index.hour\n",
    "        df_timeseries['hour_sin'] = np.sin(df_timeseries['hour'])\n",
    "        df_timeseries['hour_cos'] = np.cos(df_timeseries['hour'])\n",
    "        \n",
    "        # attach the complete dataframe to a given sensor\n",
    "        sensors[s]['dataframe'] = df_timeseries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Building the model (Pairwise approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Summary of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### dataframe to keep model prospective provenance\n",
    "summary_of_errors_pairwise = pd.DataFrame(columns=['training_sensor', 'testing_sensor', 'bin_distance', 'accuracy', 'geo_distance', 'training_sensor_region', 'testing_sensor_region'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Default Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=500, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Training and predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Remove sensor sonycnode-b827eb74a519.sonyc from the model\n",
    "\n",
    "for trainingSensor in sensors:\n",
    "        \n",
    "    print('Training sensor: ', trainingSensor)\n",
    "\n",
    "    X = sensors[trainingSensor]['dataframe'][['noise','taxi', 'wind', 'precipitation', 'hour_sin', 'hour_cos']]\n",
    "    y = sensors[trainingSensor]['dataframe'][['dbas']]\n",
    "\n",
    "    estimator, y_test, y_pred = runEstimator(X, y, classifier, 'continuous')\n",
    "\n",
    "    ## Comparing the trained model with other models\n",
    "    for testingSensor in sensors:\n",
    "        \n",
    "        if(testingSensor != trainingSensor):\n",
    "            \n",
    "            print('Testing sensor: ', testingSensor)\n",
    "\n",
    "            # plotting sensor's position.\n",
    "                # Red: sensors used to train\n",
    "                # Blue: sensors used to test\n",
    "            plotSensorsPosition(\n",
    "                [(sensors[trainingSensor]['lat'], sensors[trainingSensor]['lon'])],\n",
    "                [(sensors[testingSensor]['lat'], sensors[testingSensor]['lon'])],\n",
    "                [])\n",
    "\n",
    "            # comparing the model with another sensor's data\n",
    "            testingPredicted, testingActual = runPredictor(estimator, sensors[testingSensor])\n",
    "\n",
    "            # plotting predicted and testing timeseries in a single chart\n",
    "            test_predicted_df = plotClassifier(testingPredicted, testingActual, testingSensor)\n",
    "\n",
    "            # describing error metrics\n",
    "            accuracy, bin_distance = plotError(test_predicted_df)\n",
    "\n",
    "            # appeding error information to the dataframe\n",
    "            summary_of_errors_pairwise = summary_of_errors_pairwise.append(\n",
    "                {'training_sensor': trainingSensor, \n",
    "                 'testing_sensor': testingSensor,\n",
    "                 'bin_distance': bin_distance,\n",
    "                 'accuracy': accuracy, \n",
    "                 'geo_distance': diatenceBetweenSensors( \n",
    "                     Point(sensors[trainingSensor]['lat'], sensors[trainingSensor]['lon']),\n",
    "                     Point(sensors[testingSensor]['lat'], sensors[testingSensor]['lon'])),\n",
    "                 \n",
    "                }, ignore_index=True)                \n",
    "        \n",
    "#             # plotting taxi region\n",
    "# #             printTaxiRegionSize(trainingSensor, testingSensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Plotting error correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "alt.Chart(summary_of_errors_pairwise).mark_point().encode(\n",
    "    x='training_sensor:N',\n",
    "    y='testing_sensor:N',\n",
    "    fill='bin_distance:Q',\n",
    "    size='geo_distance:Q'\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=600\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Building the model (Permutation Approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Summary of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "summary_of_errors_permutation = pd.DataFrame(columns=['bin_distance_default', 'accuracy_default',  'bin_distance_tuning', 'accuracy_tuning', 'bin_distance_training', 'accuracy_training'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Default Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### default classifier\n",
    "classifier = RandomForestClassifier(n_estimators=500, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Training and predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for testingSensor in sensors:\n",
    "        \n",
    "    print('Testing Sensor ', testingSensor, ':')\n",
    "\n",
    "    ## dataframe to append sensors data\n",
    "    training_df = pd.DataFrame()\n",
    "    \n",
    "    ## coordinates of comparing sensors\n",
    "    training_coords = []\n",
    "    \n",
    "    for trainingSensor in sensors:        \n",
    "        \n",
    "        if(trainingSensor != testingSensor):\n",
    "            \n",
    "            ## storing trainning coords \n",
    "            training_coords.append((sensors[trainingSensor]['lat'], sensors[trainingSensor]['lon']))\n",
    "            \n",
    "            ## appending sensor data in one dataframe\n",
    "            training_df = training_df.append(sensors[trainingSensor]['dataframe'])\n",
    "            \n",
    "            \n",
    "    ## plotting sensors' position.\n",
    "#     plotSensorsPosition(training_coords, [(sensors[testingSensor]['lat'], sensors[testingSensor]['lon'])], [])\n",
    "    \n",
    "    ## spliting in X (features) and y (result)\n",
    "    X_train = training_df[['noise','taxi', 'wind', 'precipitation', 'hour_sin', 'hour_cos']]\n",
    "    y_train = training_df[['dbas']]\n",
    "    \n",
    "    ###### continuous split ######\n",
    "    \n",
    "    ## training with data for all the sensors\n",
    "    estimator, y_test, y_pred = runEstimator(X_train, y_train, classifier, 'continuous')\n",
    "    \n",
    "    ## comparing the model with another sensor's data\n",
    "    testingPredicted, testingActual = runPredictor(estimator, sensors[testingSensor])\n",
    "    \n",
    "    ## plotting both timeseries in a single chart\n",
    "    test_predicted_df = plotClassifier(testingPredicted, testingActual, testingSensor)\n",
    "        \n",
    "    ## describing error metrics\n",
    "    accuracy, bin_distance = plotError(test_predicted_df)\n",
    "    \n",
    "#     ## parameters used by the estimator\n",
    "#     importanceChart(estimator, X_train)\n",
    "\n",
    "    #############HYPERPARAMETERS ESTIMATION#############\n",
    "    \n",
    "    ## Choosing best hyperparameters using RandomizedSearchCV\n",
    "    best_estimator = bestParameters(X_train, y_train, classifier)\n",
    "    \n",
    "    ## run predictor with best parameters\n",
    "    testingPredicted_tuning, testingActual_tuning = runPredictor(best_estimator, sensors[testingSensor])\n",
    "    \n",
    "    ## plotting both timeseries in a single chart\n",
    "    test_predicted_df_tuning = plotClassifier(testingPredicted_tuning, testingActual_tuning, testingSensor)\n",
    "    \n",
    "    ## describing error metrics for hyperparameters estimation\n",
    "    accuracy_tuning, bin_distance_tuning = plotError(test_predicted_df_tuning)\n",
    "    \n",
    "    #############CROSS VALIDATION#############\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    summary_of_errors_permutation = summary_of_errors_permutation.append(\n",
    "                {'bin_distance_default': bin_distance,\n",
    "                 'accuracy_default': accuracy, \n",
    "                 'bin_distance_tuning': bin_distance_tuning,\n",
    "                 'accuracy_tuning': accuracy_tuning,\n",
    "                 'bin_distance_training':'TODO',\n",
    "                 'accuracy_training':'TODO'\n",
    "                }, ignore_index=True)    \n",
    "\n",
    "#             # filling in the error matrix with bin_distance\n",
    "#             errorMatrix_binDistance.loc[sensor][comparingSensor] = float(bin_distance)\n",
    "#             errorMatrix_accuracy.loc[sensor][comparingSensor] = float(accuracy)\n",
    "        \n",
    "#             # plotting taxi region\n",
    "#             printTaxiRegionSize(sensor, comparingSensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "barChartTuning(summary_of_errors_permutation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Building the model (Citizen Simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "summary_of_errors_citizen = pd.DataFrame(columns=['testing_sensors', 'testing_sensors_coords',  'citizen_sensors', 'citizen_sensors_coords', 'training_sensors', 'training_sensors_coords', 'bin_distance_before', 'bin_distance_after'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=500, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for testingSensor in sensors:\n",
    "        \n",
    "    print('Testing sensor ', testingSensor, ':')\n",
    "    \n",
    "    for citizenSensor in sensors:\n",
    "        \n",
    "        if(citizenSensor == testingSensor):\n",
    "            continue\n",
    "        \n",
    "        ## dataframe that will keep training features\n",
    "        training_df = pd.DataFrame()\n",
    "        \n",
    "        ## coordinates of training sensors\n",
    "        training_coords = []\n",
    "        \n",
    "        ## id of training sensors\n",
    "        training_ids = []\n",
    "        \n",
    "        for trainingSensor in sensors:        \n",
    "        \n",
    "            if( (trainingSensor != testingSensor) and (trainingSensor != citizenSensor)):\n",
    "            \n",
    "                ## storing trainning coords \n",
    "                training_coords.append((sensors[trainingSensor]['lat'], sensors[trainingSensor]['lon']))\n",
    "                \n",
    "                ## appending the name of sensors used to train\n",
    "                training_ids.append(trainingSensor)\n",
    "            \n",
    "                ## appending sensor data in one dataframe\n",
    "                training_df = training_df.append(sensors[trainingSensor]['dataframe'])\n",
    "        \n",
    "        ## plotting sensors' position.\n",
    "        plotSensorsPosition(training_coords, [(sensors[testingSensor]['lat'], sensors[testingSensor]['lon'])], [(sensors[citizenSensor]['lat'], sensors[citizenSensor]['lon'])])\n",
    "                    \n",
    "        ########### BEFORE ADDING CITIZEN SENSOR ##########    \n",
    "        \n",
    "        ## spliting in X (features) and y (result)\n",
    "        X = training_df[['noise','taxi', 'wind', 'precipitation', 'hour_sin', 'hour_cos']]\n",
    "        y = training_df[['dbas']]\n",
    "    \n",
    "        ## training with data for all the sensors\n",
    "        estimator, y_test, y_pred = runEstimator(X, y, classifier, 'continuous')\n",
    "        \n",
    "        ## comparing the model with another sensor's data\n",
    "        testingPredicted, testingActual = runPredictor(estimator, sensors[testingSensor])\n",
    "        \n",
    "        ## plotting both timeseries in a single chart\n",
    "        test_predicted_df = plotClassifier(testingPredicted, testingActual, testingSensor)\n",
    "        \n",
    "        ## calculating error for each pair (testing, citizen) before adding the citizen\n",
    "        accuracy_before, bin_distance_before = plotError(test_predicted_df)\n",
    "        \n",
    "        ########### AFTER ADDING CITIZEN SENSOR ##########  \n",
    "        \n",
    "        ## adding the citizen sensor\n",
    "        training_df = training_df.append(sensors[citizenSensor]['dataframe'])\n",
    "        \n",
    "        ## spliting in X (features) and y (result)\n",
    "        X = training_df[['noise','taxi', 'wind', 'precipitation', 'hour_sin', 'hour_cos']]\n",
    "        y = training_df[['dbas']]\n",
    "        \n",
    "        ## training with data for all the sensors\n",
    "        estimator, y_test, y_pred = runEstimator(X, y, classifier, 'continuous')\n",
    "        \n",
    "        ## comparing the model with another sensor's data\n",
    "        testingPredicted, testingActual = runPredictor(estimator, sensors[testingSensor])\n",
    "        \n",
    "        ## plotting both timeseries in a single chart\n",
    "        test_predicted_df = plotClassifier(testingPredicted, testingActual, testingSensor)\n",
    "        \n",
    "        ## calculating error for each pair (testing, citizen) before adding the citizen\n",
    "        accuracy_after, bin_distance_after = plotError(test_predicted_df)\n",
    "        \n",
    "        ## adding to the summary of errors\n",
    "        summary_of_errors_citizen = summary_of_errors_citizen.append(\n",
    "                {'testing_sensors': [testingSensor], \n",
    "                 'testing_sensors_coords': [(sensors[testingSensor]['lat'], sensors[testingSensor]['lon'])],\n",
    "                 'citizen_sensors': [citizenSensor],\n",
    "                 'citizen_sensors_coords': [(sensors[citizenSensor]['lat'], sensors[citizenSensor]['lon'])],\n",
    "                 'training_sensors': training_ids,\n",
    "                 'training_sensors_coords': training_coords,\n",
    "                 'bin_distance_before': bin_distance_before,\n",
    "                 'bin_distance_after': bin_distance_after,\n",
    "                 'accuracy_before': accuracy_before,\n",
    "                 'accuracy_after': accuracy_after,\n",
    "                 'geo_distance': diatenceBetweenSensors( \n",
    "                     Point(sensors[testingSensor]['lat'], sensors[testingSensor]['lon']),\n",
    "                     Point(sensors[citizenSensor]['lat'], sensors[citizenSensor]['lon'])),\n",
    "                }, ignore_index=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## adding bin_distance difference\n",
    "summary_of_errors_citizen['bin_distance_diff'] = summary_of_errors_citizen['bin_distance_before'] - summary_of_errors_citizen['bin_distance_after']\n",
    "summary_of_errors_citizen.sort_values(by=['geo_distance'], ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plotBarChart(summary_of_errors_citizen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tupleNumber = 21\n",
    "\n",
    "trainingSensors = summary_of_errors_citizen.loc[tupleNumber]['training_sensors_coords']\n",
    "testingSensors = summary_of_errors_citizen.loc[tupleNumber]['testing_sensors_coords']\n",
    "citizenSensors = summary_of_errors_citizen.loc[tupleNumber]['citizen_sensors_coords']\n",
    "metric = summary_of_errors_citizen.loc[tupleNumber]['bin_distance_diff']\n",
    "plotSensorsPosition(trainingSensors, testingSensors, citizenSensors, tooltip=metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model (Region Correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## default classifier\n",
    "classifier = RandomForestClassifier(n_estimators=500, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sensor in sensors:\n",
    "\n",
    "    ## dataframe to append sensors data\n",
    "    training_df = pd.DataFrame()\n",
    "    \n",
    "    ## appending sensor data in one dataframe\n",
    "    training_df = training_df.append(sensors[sensor]['dataframe']) \n",
    "    \n",
    "    ## spliting in X (features) and y (result)\n",
    "    X_train = training_df[['noise','taxi', 'wind', 'precipitation', 'hour_sin', 'hour_cos']]\n",
    "    y_train = training_df[['dbas']]\n",
    "    \n",
    "    ## training with data for all the sensors\n",
    "    estimator, y_test, y_pred = runEstimator(X_train, y_train, classifier, 'continuous')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Geospatial functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# loading taxi regions\n",
    "taxi_regions = gpd.read_file('zip://../assets/taxi_zones.zip')\n",
    "\n",
    "# translating polygon to lat/lng\n",
    "crs = {'init': 'epsg:4326'}\n",
    "taxi_regions = taxi_regions.to_crs(crs)\n",
    "\n",
    "def getSensorLocation(lat, lng):\n",
    "    ## TODO: Refactor function return\n",
    "    ## get neighboorhood ID of a coordinate\n",
    "    point = Point(lng, lat)\n",
    "    taxi_regions['intersect'] = taxi_regions.apply(lambda row: row['geometry'].intersects(point), axis=1)\n",
    "    region =  taxi_regions[taxi_regions['intersect'] == True]['LocationID']\n",
    "    return region.values[0]\n",
    "\n",
    "## Default radius\n",
    "radius = 200\n",
    "def pointWithinCircle(point, circle):\n",
    "    ## Return if a given point is within a circle\n",
    "    c = (circle[0], circle[1])\n",
    "    r = circle[2]\n",
    "    dist = geopy.distance.distance(c, point).meters\n",
    "    if dist <= r:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def diatenceBetweenSensors(sensor1, sensor2):\n",
    "    return sensor1.distance(sensor2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def runPredictor(estimator, sensor):\n",
    "    \n",
    "    Xlocal = sensor['dataframe'][['noise', 'taxi', 'wind', 'precipitation', 'hour_sin', 'hour_cos']]\n",
    "    ylocal = sensor['dataframe'][['dbas']]\n",
    "    \n",
    "    yPredLocal = estimator.predict(Xlocal)\n",
    "    \n",
    "    return yPredLocal, ylocal\n",
    "\n",
    "def runEstimator(X, y, estimator, randomize):\n",
    "\n",
    "    if(randomize == 'random'):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "        estimator.fit(X_train, y_train)\n",
    "        y_pred = estimator.predict(X_test)\n",
    "        \n",
    "    elif(randomize == 'continuous'):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
    "        estimator.fit(X_train, y_train)\n",
    "        y_pred = estimator.predict(X_test)\n",
    "        \n",
    "    return estimator, y_test, y_pred\n",
    "    \n",
    "def plotError(dataframe):\n",
    "    \n",
    "    accuracy = metrics.accuracy_score(dataframe['dbas'], dataframe['pred'])\n",
    "    bin_distance = metrics.mean_absolute_error(dataframe['dbas'],  dataframe['pred'])\n",
    "    \n",
    "    print('Accuracy: ', accuracy)\n",
    "    print('Bin Distance: ', bin_distance)\n",
    "    \n",
    "    return accuracy, bin_distance\n",
    "\n",
    "def printTaxiRegionSize(sensor, comparingSensor):\n",
    "    print(taxi_regions[taxi_regions['LocationID'] == sensors[sensor]['taxi_region']]['Shape_Area'])\n",
    "    print(taxi_regions[taxi_regions['LocationID'] == sensors[comparingSensor]['taxi_region']]['Shape_Area'])    \n",
    "    \n",
    "    \n",
    "def parameter_grid():\n",
    "    \n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "    max_depth.append(None)\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [2, 5, 10]\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "    # Method of selecting samples for training each tree\n",
    "    bootstrap = [True, False]\n",
    "    \n",
    "    hyperparameter_grid = {'n_estimators': n_estimators,\n",
    "                           'max_features': max_features,\n",
    "                           'max_depth': max_depth,\n",
    "                           'min_samples_split': min_samples_split,\n",
    "                           'min_samples_leaf': min_samples_leaf,\n",
    "                           'bootstrap': bootstrap}\n",
    "    \n",
    "    return hyperparameter_grid\n",
    "\n",
    "def bestParameters(X_train, y_train, classifier):\n",
    "\n",
    "    # get grid of hyperparameters\n",
    "    random_grid = parameter_grid()\n",
    "    \n",
    "    # declaring the random search estimator\n",
    "    rf_random = RandomizedSearchCV(estimator = classifier, param_distributions = random_grid, n_iter = 50, cv = 3, verbose=0, random_state=42, n_jobs = -1)\n",
    "    \n",
    "    # fit the random search model\n",
    "    rf_random.fit(X_train, y_train)\n",
    "    \n",
    "    return rf_random\n",
    "\n",
    "def spatialJoin(geoDataframe, lat, lon):\n",
    "    d = 200 # meters\n",
    "    n_points = 20\n",
    "    angles = np.linspace(0, 360, n_points)\n",
    "    center = shapely.geometry.Point(lon,lat)\n",
    "    polygon = geog.propagate(p, angles, d)\n",
    "    dataframe = gpd.tools.sjoin(geoDataframe,polygon,op=\"within\")\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plotSensorsPosition(trainingSensors, testingSensors, citizenSensors, tooltip='NO DATA'):\n",
    "    \n",
    "    map_osm = folium.Map(location=[40.742, -73.956], zoom_start=12, tiles=\"cartodbpositron\")\n",
    "    \n",
    "    for lat, lng in trainingSensors:\n",
    "        folium.CircleMarker(\n",
    "            location=[lat, lng],\n",
    "            radius=5,\n",
    "            fill=True,\n",
    "            fill_opacity=0.5,\n",
    "            fill_color=\"red\",\n",
    "            color=\"black\").add_to(map_osm)\n",
    "    \n",
    "    for lat, lng in testingSensors:\n",
    "        folium.CircleMarker(\n",
    "            location=[lat, lng],\n",
    "            radius=5,\n",
    "            fill=True,\n",
    "            fill_opacity=0.5,\n",
    "            fill_color=\"blue\",\n",
    "            color=\"black\").add_to(map_osm)\n",
    "        \n",
    "    for lat, lng in citizenSensors:\n",
    "        folium.CircleMarker(\n",
    "            location=[lat, lng],\n",
    "            radius=5,\n",
    "            fill=True,\n",
    "            fill_opacity=0.5,\n",
    "            fill_color=\"yellow\",\n",
    "            tooltip=tooltip,\n",
    "            color=\"black\").add_to(map_osm) \n",
    "        \n",
    "        \n",
    "    display(map_osm)\n",
    "    \n",
    "def plotClassifier(y_pred, y_test, title):\n",
    "\n",
    "    # attaching predicted values\n",
    "    y_test['pred'] = y_pred\n",
    "\n",
    "    # tranforming to long form\n",
    "    long_form_df = pd.melt(y_test.reset_index(), id_vars=['datetime'])\n",
    "    \n",
    "    brush = alt.selection(type='interval', encodings=['x'])\n",
    "\n",
    "    upper = alt.Chart(long_form_df).mark_line().encode(\n",
    "        alt.X('datetime', type='temporal', scale={'domain': brush.ref()}),\n",
    "        alt.Y('value', type='quantitative', scale=alt.Scale(zero=False)),\n",
    "        color='variable'\n",
    "    ).properties(\n",
    "        width=900\n",
    "    )\n",
    "\n",
    "    lower = upper.properties(\n",
    "        height=60\n",
    "    ).add_selection(brush)\n",
    "\n",
    "    display(alt.vconcat(upper, lower))\n",
    "    \n",
    "    return y_test\n",
    "\n",
    "def plotBarChart(dataframe):\n",
    "    \n",
    "    bar_chart = alt.Chart(dataframe.reset_index()).mark_bar().encode(\n",
    "        alt.X('index', type='nominal', sort=None),\n",
    "        alt.Y('bin_distance_diff', type='quantitative'),\n",
    "        color='geo_distance:Q',\n",
    "        tooltip=['geo_distance', 'testing_sensors', 'citizen_sensors']\n",
    "    ).properties(\n",
    "        width = 850\n",
    "    )\n",
    "    \n",
    "    display(bar_chart)\n",
    "    \n",
    "def importanceChart(estimator, train_set):\n",
    "    \n",
    "    dataframe = pd.DataFrame(estimator.feature_importances_, index=train_set.columns, columns=['importance'])\n",
    "    \n",
    "    bar_chart = alt.Chart(dataframe.reset_index()).mark_bar().encode(\n",
    "        alt.X('index', type='nominal'),\n",
    "        alt.Y('importance', type='quantitative')\n",
    "    ).properties(\n",
    "        width=500\n",
    "    )\n",
    "    \n",
    "    display(bar_chart)\n",
    "    \n",
    "    \n",
    "def barChartTuning(dataframe):\n",
    "    \n",
    "    chart_df = dataframe.drop(columns=['accuracy_default', 'accuracy_tuning', 'bin_distance_training', 'accuracy_training'])\n",
    "    \n",
    "    # tranforming to long form\n",
    "    long_form_df = pd.melt(chart_df.reset_index(), id_vars=['index'])\n",
    "    \n",
    "    bar_chart = alt.Chart(long_form_df).mark_bar().encode(\n",
    "        \n",
    "        x='variable:N',\n",
    "        y='value:Q',\n",
    "        column='index:O',\n",
    "        color='variable:N'\n",
    "        \n",
    "    )\n",
    "    \n",
    "    display(bar_chart)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
